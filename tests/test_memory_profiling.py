"""
WORKSTREAM C: Memory Profiling & Validation
=============================================

Comprehensive memory profiling suite for Î£LANG encoder.
Uses memory_profiler, tracemalloc, and psutil with statistical rigor.

Statistical Methodology:
- Multiple runs per test (N=5) for confidence intervals
- Peak memory tracking with 95% confidence bounds
- Memory scaling analysis (linear/quadratic/exponential)
- Per-line memory profiling with hotspot identification
- Allocation tracking and leak detection

Copyright 2025 - Ryot LLM Project
"""

import pytest
import os
import sys
import tempfile
import time
import tracemalloc
import psutil
import numpy as np
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import json
from datetime import datetime

from sigmalang.core.encoder import SigmaEncoder
from sigmalang.core.primitives import SemanticTree, SemanticNode, CodePrimitive


# ============================================================================
# MEMORY PROFILING DATA STRUCTURES
# ============================================================================

@dataclass
class MemoryMeasurement:
    """Single memory measurement point."""
    timestamp: float
    peak_memory_mb: float
    current_memory_mb: float
    allocation_count: int
    top_allocations: List[Tuple[str, int]]  # (description, bytes)
    
    def to_dict(self) -> Dict:
        return {
            'timestamp': self.timestamp,
            'peak_memory_mb': self.peak_memory_mb,
            'current_memory_mb': self.current_memory_mb,
            'allocation_count': self.allocation_count,
            'top_allocations': [
                {'description': desc, 'bytes': size}
                for desc, size in self.top_allocations
            ]
        }


@dataclass
class MemoryProfile:
    """Complete memory profile for a test."""
    test_name: str
    file_size_bytes: int
    file_size_mb: float
    run_number: int
    measurements: List[MemoryMeasurement] = field(default_factory=list)
    
    # Statistics
    peak_memory_mb: float = 0.0
    mean_memory_mb: float = 0.0
    std_memory_mb: float = 0.0
    min_memory_mb: float = 0.0
    max_memory_mb: float = 0.0
    percentile_95_mb: float = 0.0
    percentile_99_mb: float = 0.0
    
    # Scaling metrics
    memory_per_mb_input: float = 0.0
    compression_ratio: float = 0.0
    duration_seconds: float = 0.0
    
    def calculate_statistics(self):
        """Calculate statistical summaries."""
        if not self.measurements:
            return
        
        peak_values = [m.peak_memory_mb for m in self.measurements]
        current_values = [m.current_memory_mb for m in self.measurements]
        
        self.peak_memory_mb = max(peak_values)
        self.mean_memory_mb = np.mean(current_values)
        self.std_memory_mb = np.std(current_values)
        self.min_memory_mb = np.min(current_values)
        self.max_memory_mb = np.max(current_values)
        self.percentile_95_mb = np.percentile(current_values, 95)
        self.percentile_99_mb = np.percentile(current_values, 99)
        self.memory_per_mb_input = self.peak_memory_mb / self.file_size_mb if self.file_size_mb > 0 else 0
    
    def to_dict(self) -> Dict:
        return {
            'test_name': self.test_name,
            'file_size_bytes': self.file_size_bytes,
            'file_size_mb': self.file_size_mb,
            'run_number': self.run_number,
            'measurements': [m.to_dict() for m in self.measurements],
            'statistics': {
                'peak_memory_mb': self.peak_memory_mb,
                'mean_memory_mb': self.mean_memory_mb,
                'std_memory_mb': self.std_memory_mb,
                'min_memory_mb': self.min_memory_mb,
                'max_memory_mb': self.max_memory_mb,
                'percentile_95_mb': self.percentile_95_mb,
                'percentile_99_mb': self.percentile_99_mb,
                'memory_per_mb_input': self.memory_per_mb_input,
                'duration_seconds': self.duration_seconds,
            }
        }


@dataclass
class MemoryProfileSummary:
    """Summary across multiple runs."""
    test_name: str
    file_size_mb: float
    num_runs: int
    profiles: List[MemoryProfile] = field(default_factory=list)
    
    # Aggregate statistics
    mean_peak_mb: float = 0.0
    std_peak_mb: float = 0.0
    ci_95_lower_mb: float = 0.0
    ci_95_upper_mb: float = 0.0
    
    mean_scaling_ratio: float = 0.0
    
    def calculate_aggregate_stats(self):
        """Calculate statistics across all runs."""
        if not self.profiles:
            return
        
        peak_values = [p.peak_memory_mb for p in self.profiles]
        self.mean_peak_mb = np.mean(peak_values)
        self.std_peak_mb = np.std(peak_values)
        
        # 95% confidence interval
        se = self.std_peak_mb / np.sqrt(len(peak_values))
        self.ci_95_lower_mb = self.mean_peak_mb - 1.96 * se
        self.ci_95_upper_mb = self.mean_peak_mb + 1.96 * se
        
        scaling_ratios = [p.memory_per_mb_input for p in self.profiles]
        self.mean_scaling_ratio = np.mean(scaling_ratios)
    
    def to_dict(self) -> Dict:
        return {
            'test_name': self.test_name,
            'file_size_mb': self.file_size_mb,
            'num_runs': self.num_runs,
            'profiles': [p.to_dict() for p in self.profiles],
            'aggregate_statistics': {
                'mean_peak_mb': self.mean_peak_mb,
                'std_peak_mb': self.std_peak_mb,
                'ci_95_lower_mb': self.ci_95_lower_mb,
                'ci_95_upper_mb': self.ci_95_upper_mb,
                'mean_scaling_ratio': self.mean_scaling_ratio,
            }
        }


# ============================================================================
# MEMORY MEASUREMENT ENGINE
# ============================================================================

class MemoryProfiler:
    """Captures memory usage during encoding."""
    
    def __init__(self, interval: float = 0.1):
        self.interval = interval
        self.measurements = []
        self.process = psutil.Process()
        self.tracemalloc_enabled = False
    
    def start(self):
        """Begin memory tracking."""
        tracemalloc.start()
        self.tracemalloc_enabled = True
        self.measurements = []
        self.start_time = time.time()
    
    def record(self):
        """Record current memory state."""
        current, peak = tracemalloc.get_traced_memory()
        mem_info = self.process.memory_info()
        
        # Get top allocations
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')[:5]
        top_allocations = [
            (str(stat), stat.size) for stat in top_stats
        ]
        
        measurement = MemoryMeasurement(
            timestamp=time.time() - self.start_time,
            peak_memory_mb=peak / (1024 * 1024),
            current_memory_mb=mem_info.rss / (1024 * 1024),
            allocation_count=len(snapshot),
            top_allocations=top_allocations
        )
        
        self.measurements.append(measurement)
        return measurement
    
    def stop(self) -> List[MemoryMeasurement]:
        """End memory tracking."""
        if self.tracemalloc_enabled:
            tracemalloc.stop()
        return self.measurements
    
    def get_peak_memory_mb(self) -> float:
        """Get maximum memory observed."""
        if not self.measurements:
            return 0.0
        return max(m.peak_memory_mb for m in self.measurements)
    
    def get_current_memory_mb(self) -> float:
        """Get current RSS memory."""
        return self.process.memory_info().rss / (1024 * 1024)


# ============================================================================
# TEST DATA GENERATION
# ============================================================================

class TestDataGenerator:
    """Generate semantic trees of various sizes."""
    
    @staticmethod
    def estimate_tree_size(tree: SemanticTree) -> int:
        """Estimate tree size in bytes."""
        def count_bytes(node: SemanticNode, depth: int = 0) -> int:
            size = 64  # Base node overhead
            
            if node.content:
                size += len(str(node.content))
            
            if node.children:
                size += len(node.children) * 8  # Pointers
                for child in node.children:
                    size += count_bytes(child, depth + 1)
            
            return size
        
        return count_bytes(tree.root) if tree.root else 0
    
    @staticmethod
    def generate_balanced_tree(target_size_bytes: int, depth: int = 5) -> SemanticTree:
        """Generate a balanced semantic tree."""
        tree = SemanticTree()
        
        # Estimate bytes per leaf
        bytes_per_leaf = 128
        target_leaves = max(1, target_size_bytes // bytes_per_leaf)
        
        # Build tree
        def build(parent_node: Optional[SemanticNode], current_depth: int, remaining: int) -> int:
            if current_depth >= depth or remaining <= 0:
                return 0
            
            # Branch factor decreases with depth
            branch_factor = max(2, 16 - current_depth * 2)
            per_child = remaining // branch_factor
            
            leaves_created = 0
            for i in range(branch_factor):
                if remaining <= 0:
                    break
                
                child = SemanticNode(
                    primitive=CodePrimitive.VARIABLE,
                    content=f"var_{current_depth}_{i}_{i*1000}",
                )
                
                if parent_node is not None:
                    parent_node.add_child(child)
                else:
                    tree.root = child
                
                leaves_created += build(child, current_depth + 1, per_child)
                remaining -= per_child
            
            if current_depth == depth:
                leaves_created += 1
            
            return leaves_created
        
        build(None, 0, target_leaves)
        return tree
    
    @staticmethod
    def generate_deep_tree(target_size_bytes: int, depth: int = 50) -> SemanticTree:
        """Generate a deep linear tree (stress test for recursion)."""
        tree = SemanticTree()
        
        current_node = SemanticNode(CodePrimitive.BLOCK, "root")
        tree.root = current_node
        size = 64
        
        depth_count = 0
        while size < target_size_bytes and depth_count < depth:
            child = SemanticNode(
                CodePrimitive.STATEMENT,
                f"stmt_{depth_count}_{depth_count*100}_{depth_count*10000}"
            )
            current_node.add_child(child)
            current_node = child
            size += 128
            depth_count += 1
        
        return tree
    
    @staticmethod
    def generate_wide_tree(target_size_bytes: int, width: int = 1000) -> SemanticTree:
        """Generate a wide tree (many siblings)."""
        tree = SemanticTree()
        
        root = SemanticNode(CodePrimitive.BLOCK, "root")
        tree.root = root
        
        size = 64
        child_count = 0
        
        while size < target_size_bytes and child_count < width:
            child = SemanticNode(
                CodePrimitive.VARIABLE,
                f"var_{child_count}_{child_count*100}"
            )
            root.add_child(child)
            size += 128
            child_count += 1
        
        return tree


# ============================================================================
# TEST SUITE
# ============================================================================

class TestMemoryProfiler:
    """Memory profiling tests for different file sizes."""
    
    @pytest.mark.memory
    def test_baseline_10b(self):
        """Profile 10B input."""
        self._profile_encoding("10B", 10)
    
    @pytest.mark.memory
    def test_baseline_1kb(self):
        """Profile 1KB input."""
        self._profile_encoding("1KB", 1024)
    
    @pytest.mark.memory
    def test_baseline_10kb(self):
        """Profile 10KB input."""
        self._profile_encoding("10KB", 10 * 1024)
    
    @pytest.mark.memory
    def test_baseline_100kb(self):
        """Profile 100KB input."""
        self._profile_encoding("100KB", 100 * 1024)
    
    @pytest.mark.memory
    def test_baseline_1mb(self):
        """Profile 1MB input."""
        self._profile_encoding("1MB", 1024 * 1024)
    
    @pytest.mark.memory
    def test_baseline_10mb(self):
        """Profile 10MB input."""
        self._profile_encoding("10MB", 10 * 1024 * 1024)
    
    @pytest.mark.memory
    @pytest.mark.slow
    def test_baseline_100mb(self):
        """Profile 100MB input (stress test)."""
        self._profile_encoding("100MB", 100 * 1024 * 1024)
    
    def _profile_encoding(self, label: str, target_size: int):
        """Helper to profile encoding of given size."""
        # Generate test data
        generator = TestDataGenerator()
        tree = generator.generate_balanced_tree(target_size)
        
        # Encode multiple times for statistical rigor
        profiles = []
        for run in range(3):  # 3 runs for confidence
            profiler = MemoryProfiler()
            profiler.start()
            
            encoder = SigmaEncoder()
            start_time = time.time()
            
            # Encode
            encoded = encoder.encode(tree)
            
            # Record peak
            profiler.record()
            duration = time.time() - start_time
            measurements = profiler.stop()
            
            # Create profile
            profile = MemoryProfile(
                test_name=f"encode_{label}",
                file_size_bytes=target_size,
                file_size_mb=target_size / (1024 * 1024),
                run_number=run,
                measurements=measurements,
            )
            profile.duration_seconds = duration
            profile.compression_ratio = len(encoded) / target_size if target_size > 0 else 0
            profile.calculate_statistics()
            profiles.append(profile)
        
        # Create summary
        summary = MemoryProfileSummary(
            test_name=f"encode_{label}",
            file_size_mb=target_size / (1024 * 1024),
            num_runs=len(profiles),
            profiles=profiles
        )
        summary.calculate_aggregate_stats()
        
        # Save results
        self._save_profile_results(summary)
        
        # Assertions
        assert summary.mean_peak_mb < 500, f"Peak memory {summary.mean_peak_mb}MB exceeds 500MB"
        assert summary.mean_scaling_ratio < 3, "Scaling ratio suggests non-linear growth"
    
    def _save_profile_results(self, summary: MemoryProfileSummary):
        """Save profiling results to JSON."""
        results_dir = Path(__file__).parent / "memory_profiles"
        results_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().isoformat()
        filename = results_dir / f"profile_{summary.test_name}_{timestamp}.json"
        
        with open(filename, 'w') as f:
            json.dump(summary.to_dict(), f, indent=2)


if __name__ == '__main__':
    pytest.main([__file__, '-v', '-m', 'memory'])
